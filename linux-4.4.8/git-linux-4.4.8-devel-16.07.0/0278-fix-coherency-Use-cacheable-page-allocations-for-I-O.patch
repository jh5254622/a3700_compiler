From 3625da09f89260100cb5bf25e08825dd55570462 Mon Sep 17 00:00:00 2001
From: Ofer Heifetz <oferh@marvell.com>
Date: Sun, 13 Mar 2016 16:16:21 +0200
Subject: [PATCH 278/538] fix: coherency: Use cacheable page allocations for
 I/O buffers

ARM PL-310 L2 relies on consistent cacheable attributes.
When IOCC is enabled, all the transactions arriving from the I/Os through
the ACP port are marked with cacheable attributes.
In order to comply with the L2 restriction, The CPU must use matching
cacheable attributes.

This patch updates mvebu_hwcc_dma_ops function pointers to use coherent
dma allocations.
This change makes the buffers that are allocated for I/Os cacheable.

Signed-off-by: Nadav Haklai <nadavh@marvell.com>
Change-Id: I6ecefbecbcfe4b6bc1fb6f4ddb62f1614310d3c9
Signed-off-by: Ofer Heifetz <oferh@marvell.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/28263
Reviewed-by: Lior Amsalem <alior@marvell.com>
Tested-by: Lior Amsalem <alior@marvell.com>
---
 arch/arm/include/asm/dma-mapping.h |  6 ++++++
 arch/arm/mach-mvebu/coherency.c    |  8 ++++----
 arch/arm/mm/dma-mapping.c          | 34 +++++++++++++---------------------
 3 files changed, 23 insertions(+), 25 deletions(-)

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index ccb3aa6..2051c91 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -185,6 +185,9 @@ extern int arm_dma_set_mask(struct device *dev, u64 dma_mask);
 extern void *arm_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,
 			   gfp_t gfp, struct dma_attrs *attrs);
 
+extern void *arm_coherent_dma_alloc(struct device *dev, size_t size,
+				    dma_addr_t *handle, gfp_t gfp, struct dma_attrs *attrs);
+
 /**
  * arm_dma_free - free memory allocated by arm_dma_alloc
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
@@ -202,6 +205,9 @@ extern void *arm_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,
 extern void arm_dma_free(struct device *dev, size_t size, void *cpu_addr,
 			 dma_addr_t handle, struct dma_attrs *attrs);
 
+extern void arm_coherent_dma_free(struct device *dev, size_t size, void *cpu_addr,
+				  dma_addr_t handle, struct dma_attrs *attrs);
+
 /**
  * arm_dma_mmap - map a coherent DMA allocation into user space
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
diff --git a/arch/arm/mach-mvebu/coherency.c b/arch/arm/mach-mvebu/coherency.c
index 48807d0..776093b 100644
--- a/arch/arm/mach-mvebu/coherency.c
+++ b/arch/arm/mach-mvebu/coherency.c
@@ -125,9 +125,9 @@ static void mvebu_hwcc_dma_sync(struct device *dev, dma_addr_t dma_handle,
 		mvebu_hwcc_sync_io_barrier();
 }
 
-static const struct dma_map_ops mvebu_hwcc_dma_ops = {
-	.alloc			= arm_dma_alloc,
-	.free			= arm_dma_free,
+static struct dma_map_ops mvebu_hwcc_dma_ops = {
+	.alloc			= arm_coherent_dma_alloc,
+	.free			= arm_coherent_dma_free,
 	.mmap			= arm_dma_mmap,
 	.map_page		= mvebu_hwcc_dma_map_page,
 	.unmap_page		= mvebu_hwcc_dma_unmap_page,
@@ -138,7 +138,7 @@ static const struct dma_map_ops mvebu_hwcc_dma_ops = {
 	.sync_single_for_device	= mvebu_hwcc_dma_sync,
 	.sync_sg_for_cpu	= arm_dma_sync_sg_for_cpu,
 	.sync_sg_for_device	= arm_dma_sync_sg_for_device,
-	.set_dma_mask	= arm_dma_set_mask,
+	.set_dma_mask		= arm_dma_set_mask,
 };
 
 static int mvebu_hwcc_notifier(struct notifier_block *nb,
diff --git a/arch/arm/mm/dma-mapping.c b/arch/arm/mm/dma-mapping.c
index 534a60a..c5b676c 100644
--- a/arch/arm/mm/dma-mapping.c
+++ b/arch/arm/mm/dma-mapping.c
@@ -145,25 +145,6 @@ struct dma_map_ops arm_dma_ops = {
 };
 EXPORT_SYMBOL(arm_dma_ops);
 
-static void *arm_coherent_dma_alloc(struct device *dev, size_t size,
-	dma_addr_t *handle, gfp_t gfp, struct dma_attrs *attrs);
-static void arm_coherent_dma_free(struct device *dev, size_t size, void *cpu_addr,
-				  dma_addr_t handle, struct dma_attrs *attrs);
-static int arm_coherent_dma_mmap(struct device *dev, struct vm_area_struct *vma,
-		 void *cpu_addr, dma_addr_t dma_addr, size_t size,
-		 struct dma_attrs *attrs);
-
-struct dma_map_ops arm_coherent_dma_ops = {
-	.alloc			= arm_coherent_dma_alloc,
-	.free			= arm_coherent_dma_free,
-	.mmap			= arm_coherent_dma_mmap,
-	.get_sgtable		= arm_dma_get_sgtable,
-	.map_page		= arm_coherent_dma_map_page,
-	.map_sg			= arm_dma_map_sg,
-	.set_dma_mask		= arm_dma_set_mask,
-};
-EXPORT_SYMBOL(arm_coherent_dma_ops);
-
 static int __dma_supported(struct device *dev, u64 mask, bool warn)
 {
 	unsigned long max_dma_pfn;
@@ -681,7 +662,7 @@ void *arm_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,
 			   attrs, __builtin_return_address(0));
 }
 
-static void *arm_coherent_dma_alloc(struct device *dev, size_t size,
+void *arm_coherent_dma_alloc(struct device *dev, size_t size,
 	dma_addr_t *handle, gfp_t gfp, struct dma_attrs *attrs)
 {
 	return __dma_alloc(dev, size, handle, gfp, PAGE_KERNEL, true,
@@ -768,7 +749,7 @@ void arm_dma_free(struct device *dev, size_t size, void *cpu_addr,
 	__arm_dma_free(dev, size, cpu_addr, handle, attrs, false);
 }
 
-static void arm_coherent_dma_free(struct device *dev, size_t size, void *cpu_addr,
+void arm_coherent_dma_free(struct device *dev, size_t size, void *cpu_addr,
 				  dma_addr_t handle, struct dma_attrs *attrs)
 {
 	__arm_dma_free(dev, size, cpu_addr, handle, attrs, true);
@@ -2122,6 +2103,17 @@ static void arm_teardown_iommu_dma_ops(struct device *dev) { }
 
 #endif	/* CONFIG_ARM_DMA_USE_IOMMU */
 
+struct dma_map_ops arm_coherent_dma_ops = {
+	.alloc		= arm_coherent_dma_alloc,
+	.free		= arm_coherent_dma_free,
+	.mmap		= arm_coherent_dma_mmap,
+	.get_sgtable	= arm_dma_get_sgtable,
+	.map_page	= arm_coherent_dma_map_page,
+	.map_sg		= arm_dma_map_sg,
+	.set_dma_mask	= arm_dma_set_mask,
+};
+EXPORT_SYMBOL(arm_coherent_dma_ops);
+
 static struct dma_map_ops *arm_get_dma_map_ops(bool coherent)
 {
 	return coherent ? &arm_coherent_dma_ops : &arm_dma_ops;
-- 
1.9.1

