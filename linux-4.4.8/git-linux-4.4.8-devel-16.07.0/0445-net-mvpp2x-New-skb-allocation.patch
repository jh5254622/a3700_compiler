From 579bbd77942e48b6393c21f916776deb6b059298 Mon Sep 17 00:00:00 2001
From: Stefan Chulski <stefanc@marvell.com>
Date: Tue, 24 May 2016 18:54:14 +0300
Subject: [PATCH 445/538] net: mvpp2x: New skb allocation.

-skb is built with build_skb()

Change-Id: If74363cdb4fdcc013cbabff1a16c8247551742c3
Reviewed-on: http://vgitil04.il.marvell.com:8080/29949
Reviewed-by: Yuval Caduri <cyuval@marvell.com>
Reviewed-by: Hanna Hawa <hannah@marvell.com>
Tested-by: Hanna Hawa <hannah@marvell.com>
---
 drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.c   |   2 +-
 drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.h   |  14 +-
 .../net/ethernet/marvell/mvpp2x/mv_pp2x_hw_type.h  |   4 +
 drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c | 162 +++++++++++++--------
 4 files changed, 112 insertions(+), 70 deletions(-)

diff --git a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.c b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.c
index 7b536ef..f055815d 100644
--- a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.c
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.c
@@ -3943,7 +3943,7 @@ void mv_pp21_port_reset(struct mv_pp2x_port *port)
 
 /* Refill BM pool */
 void mv_pp2x_pool_refill(struct mv_pp2x *priv, u32 pool,
-			 dma_addr_t phys_addr, struct sk_buff *cookie)
+			 dma_addr_t phys_addr, u8 *cookie)
 {
 	STAT_DBG(struct mv_pp2x_bm_pool *bm_pool = &priv->bm_pools[pool]);
 
diff --git a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.h b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.h
index 9ef209e..d02736a 100644
--- a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.h
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.h
@@ -283,7 +283,7 @@ static inline void mv_pp2x_bm_hw_pool_create(struct mv_pp2x_hw *hw,
 /* Release buffer to BM */
 static inline void mv_pp2x_bm_pool_put(struct mv_pp2x_hw *hw, u32 pool,
 					      dma_addr_t buf_phys_addr,
-					      struct sk_buff *buf_virt_addr)
+					      u8 *buf_virt_addr)
 {
 
 	mv_pp2x_relaxed_write(hw, MVPP2_BM_VIRT_RLS_REG,
@@ -309,7 +309,7 @@ static inline void mv_pp2x_bm_pool_mc_put(struct mv_pp2x_port *port, int pool,
 	mv_pp2x_bm_pool_put(&(port->priv->hw), pool,
 			    (dma_addr_t)(buf_phys_addr |
 			    MVPP2_BM_PHY_RLS_MC_BUFF_MASK),
-			    (struct sk_buff *)(u64)(buf_virt_addr));
+			    (u8 *)(u64)(buf_virt_addr));
 }
 
 static inline void mv_pp2x_port_interrupts_enable(struct mv_pp2x_port *port)
@@ -407,10 +407,10 @@ static inline void mv_pp2x_txq_sent_counter_clear(void *arg)
 	}
 }
 
-static inline struct sk_buff *mv_pp21_rxdesc_cookie_get(
+static inline u8 *mv_pp21_rxdesc_cookie_get(
 		struct mv_pp2x_rx_desc *rx_desc)
 {
-	return((struct sk_buff *)((uintptr_t)rx_desc->u.pp21.buf_cookie));
+	return((u8 *)((uintptr_t)rx_desc->u.pp21.buf_cookie));
 }
 
 static inline dma_addr_t mv_pp21_rxdesc_phys_addr_get(
@@ -420,10 +420,10 @@ static inline dma_addr_t mv_pp21_rxdesc_phys_addr_get(
 }
 
 /*YuvalC: Below functions are intended to support both aarch64 & aarch32 */
-static inline struct sk_buff *mv_pp22_rxdesc_cookie_get(
+static inline u8 *mv_pp22_rxdesc_cookie_get(
 		struct mv_pp2x_rx_desc *rx_desc)
 {
-	return((struct sk_buff *)((uintptr_t)
+	return((u8 *)((uintptr_t)
 		(rx_desc->u.pp22.buf_cookie_bm_qset_cls_info &
 		DMA_BIT_MASK(40))));
 }
@@ -563,7 +563,7 @@ void mv_pp2x_bm_pool_bufsize_set(struct mv_pp2x_hw *hw,
 					 struct mv_pp2x_bm_pool *bm_pool,
 					 int buf_size);
 void mv_pp2x_pool_refill(struct mv_pp2x *priv, u32 pool,
-			    dma_addr_t phys_addr, struct sk_buff *cookie);
+			    dma_addr_t phys_addr, u8 *cookie);
 
 void mv_pp21_rxq_long_pool_set(struct mv_pp2x_hw *hw,
 				     int prxq, int long_pool);
diff --git a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw_type.h b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw_type.h
index 9020134..da5e5bb 100644
--- a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw_type.h
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw_type.h
@@ -2247,6 +2247,7 @@ struct mv_pp2x_bm_pool {
 	int buf_size;
 	/* Packet size */
 	int pkt_size;
+	int frag_size;
 
 	/* BPPE virtual base address */
 	void *virt_addr;
@@ -2263,6 +2264,9 @@ struct mv_pp2x_bm_pool {
 #ifdef MVPP2_DEBUG
 	struct mv_pp2x_bm_pool_stats stats;
 #endif
+#ifdef CONFIG_64BIT
+	u64 data_high;
+#endif
 };
 
 struct mv_pp2x_buff_hdr {
diff --git a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c
index 3e195dd..d26f19f 100644
--- a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c
@@ -251,6 +251,53 @@ static inline const char *
 }
 
 /* Buffer Manager configuration routines */
+static void *mv_pp2x_frag_alloc(const struct mv_pp2x_bm_pool *pool)
+{
+	if (likely(pool->frag_size <= PAGE_SIZE))
+		return netdev_alloc_frag(pool->frag_size);
+	else
+		return kmalloc(pool->frag_size, GFP_ATOMIC);
+}
+
+static void mv_pp2x_frag_free(const struct mv_pp2x_bm_pool *pool, void *data)
+{
+	if (likely(pool->frag_size <= PAGE_SIZE))
+		skb_free_frag(data);
+	else
+		kfree(data);
+}
+
+static int mv_pp2x_rx_refill_new(struct mv_pp2x_port *port,
+			   struct mv_pp2x_bm_pool *bm_pool,
+			   u32 pool, int is_recycle)
+{
+	dma_addr_t phys_addr;
+	void *data;
+
+	if (is_recycle &&
+	    (atomic_read(&bm_pool->in_use) < bm_pool->in_use_thresh))
+		return 0;
+
+	data = mv_pp2x_frag_alloc(bm_pool);
+	if (!data)
+		return -ENOMEM;
+#ifdef CONFIG_64BIT
+	if (unlikely(bm_pool->data_high != ((u64)data & 0xffffffff00000000)))
+		return -ENOMEM;
+#endif
+
+	phys_addr = dma_map_single(port->dev->dev.parent, data,
+				   MVPP2_RX_BUF_SIZE(bm_pool->pkt_size),
+				   DMA_FROM_DEVICE);
+	if (unlikely(dma_mapping_error(port->dev->dev.parent, phys_addr))) {
+		mv_pp2x_frag_free(bm_pool, data);
+		return -ENOMEM;
+	}
+
+	mv_pp2x_pool_refill(port->priv, pool, phys_addr, (u8 *)data);
+	atomic_dec(&bm_pool->in_use);
+	return 0;
+}
 
 /* Create pool */
 static int mv_pp2x_bm_pool_create(struct device *dev,
@@ -287,10 +334,23 @@ static int mv_pp2x_bm_pool_create(struct device *dev,
 
 	bm_pool->size = size;
 	bm_pool->pkt_size = mv_pp2x_pool_pkt_size_get(bm_pool->log_id);
+	bm_pool->frag_size = SKB_DATA_ALIGN(MVPP2_RX_BUF_SIZE(bm_pool->pkt_size)) +
+				MVPP2_SKB_SHINFO_SIZE;
 	bm_pool->buf_num = 0;
 	mv_pp2x_bm_pool_bufsize_set(hw, bm_pool,
 				    MVPP2_RX_BUF_SIZE(bm_pool->pkt_size));
 	atomic_set(&bm_pool->in_use, 0);
+#ifdef CONFIG_64BIT
+{
+	void *data_tmp;
+
+	data_tmp = mv_pp2x_frag_alloc(bm_pool);
+	if (data_tmp) {
+		bm_pool->data_high = (u64)data_tmp & 0xffffffff00000000;
+		mv_pp2x_frag_free(bm_pool, data_tmp);
+	}
+}
+#endif
 
 	return 0;
 }
@@ -461,50 +521,11 @@ static int mv_pp2x_bm_init(struct platform_device *pdev, struct mv_pp2x *priv)
 	return 0;
 }
 
-/* Allocate skb for BM pool */
-static struct sk_buff *mv_pp2x_skb_alloc(struct mv_pp2x_port *port,
-					      struct mv_pp2x_bm_pool *bm_pool,
-					      dma_addr_t *buf_phys_addr,
-					      gfp_t gfp_mask)
-{
-	struct sk_buff *skb;
-	dma_addr_t phys_addr;
-
-	gfp_mask |= GFP_DMA;
-	skb = __dev_alloc_skb(bm_pool->pkt_size, gfp_mask);
-	if (!skb) {
-		pr_crit_once("%s skb alloc failed\n", __func__);
-		STAT_DBG(bm_pool->stats.skb_alloc_oom++);
-		return NULL;
-	}
-	STAT_DBG(bm_pool->stats.skb_alloc_ok++);
-	phys_addr = dma_map_single(port->dev->dev.parent, skb->head,
-				   MVPP2_RX_BUF_SIZE(bm_pool->pkt_size),
-				   DMA_FROM_DEVICE);
-	pr_debug_once("dev_ptr:%p, dev_name:%s, sizeof(dma_addr_t):%ld",
-		      port->dev->dev.parent, port->dev->dev.parent->init_name,
-		      sizeof(dma_addr_t));
-	pr_debug_once("sizeof(long):%ld, phys_addr:%llx, size:%d\n",
-		      sizeof(long),
-		      phys_addr, MVPP2_RX_BUF_SIZE(bm_pool->pkt_size));
-
-	if (unlikely(dma_mapping_error(port->dev->dev.parent, phys_addr))) {
-		MVPP2_PRINT_2LINE();
-		dev_kfree_skb_any(skb);
-		return NULL;
-	}
-	*buf_phys_addr = phys_addr;
-
-	return skb;
-}
-
 /* Allocate buffers for the pool */
 int mv_pp2x_bm_bufs_add(struct mv_pp2x_port *port,
 			       struct mv_pp2x_bm_pool *bm_pool, int buf_num)
 {
-	struct sk_buff *skb;
 	int i, buf_size, total_size;
-	dma_addr_t phys_addr;
 
 	buf_size = MVPP2_RX_BUF_SIZE(bm_pool->pkt_size);
 	total_size = MVPP2_RX_TOTAL_SIZE(buf_size);
@@ -519,13 +540,8 @@ int mv_pp2x_bm_bufs_add(struct mv_pp2x_port *port,
 
 	MVPP2_PRINT_VAR(buf_num);
 
-	for (i = 0; i < buf_num; i++) {
-		skb = mv_pp2x_skb_alloc(port, bm_pool, &phys_addr, GFP_KERNEL);
-		if (!skb)
-			break;
-		mv_pp2x_pool_refill(port->priv, (u32)bm_pool->id,
-				    phys_addr, skb);
-	}
+	for (i = 0; i < buf_num; i++)
+		mv_pp2x_rx_refill_new(port, bm_pool, (u32)bm_pool->id, 0);
 
 	/* Update BM driver with number of buffers added to pool */
 	bm_pool->buf_num += i;
@@ -1001,7 +1017,7 @@ static void mv_pp2x_rxq_drop_pkts(struct mv_pp2x_port *port,
 					struct mv_pp2x_rx_queue *rxq)
 {
 	int rx_received, i;
-	struct sk_buff *buf_cookie;
+	u8 *buf_cookie;
 	dma_addr_t buf_phys_addr;
 
 	rx_received = mv_pp2x_rxq_received(port, rxq->id);
@@ -2017,7 +2033,7 @@ int mv_pp22_rss_default_cpu_set(struct mv_pp2x_port *port, int default_cpu)
 }
 
 /* Main RX/TX processing routines */
-
+#if 0
 
 /* Reuse skb if possible, or allocate a new skb and add it to BM pool */
 static int mv_pp2x_rx_refill(struct mv_pp2x_port *port,
@@ -2040,6 +2056,8 @@ static int mv_pp2x_rx_refill(struct mv_pp2x_port *port,
 	atomic_dec(&bm_pool->in_use);
 	return 0;
 }
+}
+#endif
 
 /* Handle tx checksum */
 static u32 mv_pp2x_skb_tx_csum(struct mv_pp2x_port *port, struct sk_buff *skb)
@@ -2145,6 +2163,7 @@ static int mv_pp2x_rx(struct mv_pp2x_port *port, struct napi_struct *napi,
 		u32 rx_status, pool;
 		int rx_bytes, err;
 		dma_addr_t buf_phys_addr;
+		unsigned char *data;
 
 #if defined(__BIG_ENDIAN)
 		if (port->priv->pp2_version == PPV21)
@@ -2166,16 +2185,15 @@ static int mv_pp2x_rx(struct mv_pp2x_port *port, struct napi_struct *napi,
 		}
 
 		if (port->priv->pp2_version == PPV21) {
-			skb = mv_pp21_rxdesc_cookie_get(rx_desc);
+			data = mv_pp21_rxdesc_cookie_get(rx_desc);
 			buf_phys_addr = mv_pp21_rxdesc_phys_addr_get(rx_desc);
 		} else {
-			skb = mv_pp22_rxdesc_cookie_get(rx_desc);
+			data = mv_pp22_rxdesc_cookie_get(rx_desc);
 			buf_phys_addr = mv_pp22_rxdesc_phys_addr_get(rx_desc);
 		}
 
 #ifdef CONFIG_64BIT
-		skb = (struct sk_buff *)((uintptr_t)skb |
-			port->priv->pp2xdata->skb_base_addr);
+		data = (unsigned char *)((uintptr_t)data | bm_pool->data_high);
 #endif
 
 #ifdef MVPP2_VERBOSE
@@ -2193,29 +2211,49 @@ static int mv_pp2x_rx(struct mv_pp2x_port *port, struct napi_struct *napi,
 		 */
 		if (rx_status & MVPP2_RXD_ERR_SUMMARY) {
 			pr_err("MVPP2_RXD_ERR_SUMMARY\n");
+err_drop_frame:
 			dev->stats.rx_errors++;
 			mv_pp2x_rx_error(port, rx_desc);
 			mv_pp2x_pool_refill(port->priv, pool, buf_phys_addr,
-				skb);
+				data);
 			continue;
 		}
 
-		rcvd_pkts++;
-		rcvd_bytes += rx_bytes;
 		atomic_inc(&bm_pool->in_use);
+		err = mv_pp2x_rx_refill_new(port, bm_pool, pool, 0);
+		if (err) {
+			netdev_err(port->dev, "failed to refill BM pools\n");
+			rx_filled--;
+		}
 
-		skb_reserve(skb, MVPP2_MH_SIZE);
+		skb = build_skb(data, bm_pool->frag_size > PAGE_SIZE ? 0 : bm_pool->frag_size);
+		/* After refill old buffer has to be unmapped regardless
+		 * the skb is successfully built or not.
+		 */
+		dma_unmap_single(dev->dev.parent, buf_phys_addr,
+				 MVPP2_RX_BUF_SIZE(bm_pool->pkt_size), DMA_FROM_DEVICE);
+
+		if (!skb) {
+			pr_err("skb build failed\n");
+			goto err_drop_frame;
+		}
+
+#ifdef MVPP2_VERBOSE
+		mv_pp2x_skb_dump(skb, rx_desc->data_size, 4);
+#endif
+#if 0
+		dma_sync_single_for_cpu(dev->dev.parent, buf_phys_addr,
+					MVPP2_RX_BUF_SIZE(rx_desc->data_size),
+					DMA_FROM_DEVICE);
+#endif
+		rcvd_pkts++;
+		rcvd_bytes += rx_bytes;
+		skb_reserve(skb, MVPP2_MH_SIZE+NET_SKB_PAD);
 		skb_put(skb, rx_bytes);
 		skb->protocol = eth_type_trans(skb, dev);
 		mv_pp2x_rx_csum(port, rx_status, skb);
 
 		napi_gro_receive(napi, skb);
-
-		err = mv_pp2x_rx_refill(port, bm_pool, pool, 0);
-		if (err) {
-			netdev_err(port->dev, "failed to refill BM pools\n");
-			rx_filled--;
-		}
 	}
 
 	if (rcvd_pkts) {
-- 
1.9.1

